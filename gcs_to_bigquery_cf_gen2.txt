import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions

# Define your GCS and BigQuery configurations
project_id = 'gcp-to-bigquery-schemav'
output_table = 'gcp-to-bigquery-schema.Test2.Employee'

# Define pipeline options
options = PipelineOptions()
google_cloud_options = options.view_as(GoogleCloudOptions)
google_cloud_options.project = project_id
google_cloud_options.region = 'us-central1'
google_cloud_options.temp_location = 'gs://temp_store_alti/temp/'

schema = {
    'fields': [
        {'name': 'EMPLOYEE_ID', 'type': 'INTEGER'},
        {'name': 'FIRST_NAME', 'type': 'STRING'},
        {'name': 'LAST_NAME', 'type': 'STRING'},
        {'name': 'EMAIL', 'type': 'STRING'},
        {'name': 'PHONE_NUMBER', 'type': 'INTEGER'},
        {'name': 'HIRE_DATE', 'type': 'STRING'},
        {'name': 'JOB_ID', 'type': 'STRING'},
        {'name': 'SALARY', 'type': 'FLOAT'},
        {'name': 'COMMISSION_PCT', 'type': 'STRING'},
        {'name': 'MANAGER_ID', 'type': 'INTEGER'},
        {'name': 'DEPARTMENT_ID', 'type': 'INTEGER'}
    ]
}

class ParseCSV(beam.DoFn):
    def process(self, element):
        fields = element.split(',')
        try:
            parsed_record = {
                'EMPLOYEE_ID': int(fields[0]),
                'FIRST_NAME': fields[1],
                'LAST_NAME': fields[2],
                'EMAIL' : fields[3],
                'PHONE_NUMBER' : int(fields[4]),
                'HIRE_DATE': fields[5],
                'JOB_ID' : fields[6],
                'SALARY' : float(fields[7]),
                'COMMISSION_PCT' : fields[8],
                'MANAGER_ID' : int(fields[9]),
                'DEPARTMENT_ID' : int(fields[10])
            }
            yield parsed_record
        except (IndexError, ValueError):
            yield beam.pvalue.TaggedOutput('failed_records', element)

def process_pipeline(bucket_and_file):
    bucket, name = bucket_and_file
    input_file = f'gs://{bucket}/{name}'

    # Define the pipeline inside the process function
    with beam.Pipeline(options=options) as pipeline:
        # Step 2: Read data from GCS
        lines = pipeline | 'ReadFromGCS' >> beam.io.ReadFromText(input_file)
        
        separated_data = lines | beam.ParDo(ParseCSV()).with_outputs(
            'failed_records', main='main'
        )

        main_records = separated_data.main
        failed_records = separated_data.failed_records

        # Write valid records to BigQuery
        main_records | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(
            output_table,
            schema=schema,
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
        )

        # Write invalid records to GCS
        non_matching_output_path = f'gs://{bucket}/non_matching_records.txt'
        failed_records | 'WriteFailedToGCS' >> beam.io.WriteToText(non_matching_output_path)

def run():
    with beam.Pipeline(options=options) as pipeline:
        # Step 1: Read bucket name and file name from BigQuery with a subquery
        query = """
        SELECT 
            bucket, 
            name
        FROM (
            SELECT 
                bucket, 
                name, 
                MAX(time_created) AS time_created
            FROM `gcp-to-bigquery-schemav.gcs_dataset.gcs_files`
            GROUP BY bucket, name
        )
        WHERE time_created = (SELECT MAX(time_created) FROM `gcp-to-bigquery-schemav.gcs_dataset.gcs_files`)
        LIMIT 1
        """
        bq_values = (
            pipeline
            | "Read Bucket and File from BigQuery" >> beam.io.ReadFromBigQuery(query=query, use_standard_sql=True)
            | "Extract Bucket and File" >> beam.Map(lambda x: (x['bucket'], x['name']))
        )

        # Step 3: Apply the pipeline processing to the results from BigQuery
        bq_values | "Process Pipeline" >> beam.Map(process_pipeline)

if __name__ == '__main__':
    run()
